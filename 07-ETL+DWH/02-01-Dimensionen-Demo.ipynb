{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/fau-lmi/lct-ehealth-datawarehouse/blob/main/01-01-Extraktion-Demo.ipynb","timestamp":1714911186628},{"file_id":"1BL6fz2-jOCEmlbHL_LUVxhtHzK709prp","timestamp":1714904625208}],"authorship_tag":"ABX9TyNIrajyRMhTEYANx1tO8wTV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 02-01 Dimensionen Demo\n","\n","## Hinweise zur Übung\n","\n","Ziel dieser Übung ist die beispielhafte Erzeugung von Dimensionstabellen für das Data Warehouse-Szenario. Hierzu nutzen wir die Inhalte der Staging-Datenbanken als Datenquelle und transformieren daraus die Dimensionsdaten und liefern sie in die Reporting-Datenbank aus.\n","\n","Die Übung setzt auf den Ergebnissen der Extraktions (Übung 01) auf. Da in der Übung nur einzelne Tabellen in die Staging-Datenbanken übertragen wurden, setzen wir hier mit einem einheitlichen Stand aus vorbereiteten SQLite-Datenbanken auf, in die alle TSV-Dateien bereits importiert wurden.\n","\n","Die Tabellenstruktur der zu erstellenden Dimensionstabellen inkl. der Spalten (nicht jedoch Datentypen) geht aus dem ER-Diagramm im `data`-Verzeichnis hervor. Die Datentypen wiederum können in den Dokumentationen der jeweiligen Qeulldatenbereiche (Bielefeld, Mannheim, Dimensionen) nachgeschlagen werden.\n","\n","## Nutzung des ELT- statt ETL-Ansatzes\n","\n","In dieser Übung laden wir Daten nicht mehr aus CSV-Dateien, sondern zwischen Schemata (hier in SQLite: Teil-Datenbanken) der gleichen übergeordneten Datenbank hin und her.\n","\n","Hierzu können wir die SQL Funktionen `CREATE TABLE ... AS SELECT ...` bzw. `INSERT INTO ... SELECT ...` nutzen, um Daten direkt von einem Teil der Datenbank zu lesen und in eine andere Tabelle der gleichen Datenbank (hier in einem anderen Schema) zu schreiben.\n","\n","Da hierbei kein ETL-Werkzeug benötigt wird, das die Teilschritte des extrahierens, transformierens und ladens getrennt voneinander ausführt, wird diese Vorgehensweise auch als **ELT** (Extract, Load **and** Transform) bezeichnet. Wir setzen entsprechend in diesem Notebook nicht PETL, sondern nur SQL ein."],"metadata":{"id":"SvxhKbdnO7j3"}},{"cell_type":"markdown","source":["## Konfiguration des Notebooks"],"metadata":{"id":"bbsb3U9UkAl1"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"1D0zfQ-IzSGk"},"outputs":[],"source":["# Ggf. fehlende Pakete installieren\n","!pip install --quiet ipython-sql"]},{"cell_type":"code","source":["import os\n","import sys\n","import urllib.request\n","%load_ext sql"],"metadata":{"id":"44KM1u1Zzn3-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Konfiguration\n","base_url_quellen = \"https://raw.githubusercontent.com/fau-lmi/lct-ehealth/main/07-ETL+DWH/data\"\n","base_url_staging = \"./\""],"metadata":{"id":"MP1zDb_s0-rq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# SQlite-Datenbanken aus Github auf den Jupyter-Server herunterladen\n","urllib.request.urlretrieve(base_url_quellen + \"/datenbanken/stg_bielefeld.sqlite\",  \"stg_bielefeld.sqlite\")\n","urllib.request.urlretrieve(base_url_quellen + \"/datenbanken/stg_mannheim.sqlite\",   \"stg_mannheim.sqlite\")\n","urllib.request.urlretrieve(base_url_quellen + \"/datenbanken/stg_dimensions.sqlite\", \"stg_dimensions.sqlite\")\n","urllib.request.urlretrieve(base_url_quellen + \"/datenbanken/staging.sqlite\",        \"staging.sqlite\")\n"],"metadata":{"id":"qs1-4gbkrjSO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Datenbankverbindung als Pfad (für das ETL) & iPython SQL (für die Abfragen) herstellen\n","db_path_stg_bielefeld  = base_url_staging + \"stg_bielefeld.sqlite\"\n","db_path_stg_mannheim   = base_url_staging + \"stg_mannheim.sqlite\"\n","db_path_stg_dimensions = base_url_staging + \"stg_dimensions.sqlite\"\n","db_path_staging        = base_url_staging + \"staging.sqlite\"\n","db_path_reporting      = base_url_staging + \"reporting.sqlite\"\n","\n","db_url_stg_bielefeld  = \"sqlite:///\" + db_path_stg_bielefeld\n","db_url_stg_mannheim   = \"sqlite:///\" + db_path_stg_mannheim\n","db_url_stg_dimensions = \"sqlite:///\" + db_path_stg_dimensions\n","db_url_staging        = \"sqlite:///\" + db_path_staging\n","db_url_reporting      = \"sqlite:///\" + db_path_reporting\n","\n","%sql $db_url_stg_bielefeld\n","%sql $db_url_stg_mannheim\n","%sql $db_url_stg_dimensions\n","%sql $db_url_staging\n","%sql $db_url_reporting"],"metadata":{"id":"-vj4aaez2LXG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%sql $db_url_staging\n","ATTACH DATABASE :db_path_stg_bielefeld  AS bielefeld;\n","ATTACH DATABASE :db_path_stg_mannheim   AS mannheim;\n","ATTACH DATABASE :db_path_stg_dimensions AS dimensions;\n","ATTACH DATABASE :db_path_reporting      AS reporting;"],"metadata":{"id":"Qpl48MBD2LfC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Dimensionstabelle D_AUFNAHMEGRUND\n","\n","Der Aufnahmegrund ist eine wichtige Information für die Auswertung klinischer Versorgungsdaten (\"Warum wurde ein Patient aufgenommen?\"). Im deutschen Gesundheitswesen findet man hierzu hauptsächlich abrechnungsrelevante Daten, während der in der Übung verwendete Synthea-Datensatz eher medizinische Gründe listet.\n","\n","Im folgenden Block erzeugen wir die Dimensionstabelle D_AUFNAHMEGRUND. Ihre Quelldaten können wir direkt aus den Rohdatentabellen FAELLE/ENCOUNTER der beiden Quelldatenbanken entnehmen. Sie enthalten SNOMED-Codes und -Bezeichner für die Aufnahmegründe. Da in beiden Quelldatenbanken der gleiche Katalog (SNOMED CT) für die Aufnahmegründe verwendet wird, können wir eine gemeinsame Dimension mit den gleichen Identifiern anlegen.\n","\n","Da diese Dimension nur eine Ebene mit wenigen Einträgen hat, gibt es keine Hierarchie, und sowohl die Benennung der Spalten als auch die Abfragen zur Generierung sind sehr überschaubar."],"metadata":{"id":"mfEtqN2sne-F"}},{"cell_type":"markdown","source":["### 1. Zieltabelle anlegen\n","\n","Hierzu legen wir zunächst mit einem CREATE-Table die Tabelle in der Reporting-Datenbank (leer) an."],"metadata":{"id":"XqNWZD0T20m0"}},{"cell_type":"code","source":["%%sql $db_url_staging\n","DROP TABLE IF EXISTS reporting.d_aufnahmegrund;\n","\n","CREATE TABLE reporting.d_aufnahmegrund (\n","  aufnahmegrund_id   VARCHAR(20),\n","  aufnahmegrund_name VARCHAR(100)\n",");"],"metadata":{"id":"-bb8aZtj-nFA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2. Daten aus den Quelltabellen auslesen & in Zieltabelle schreiben\n","\n","In diesem Schritt lesen wir mit `SELECT`-Statements die Daten aus den Quelltabellen beider Standorte aus.\n","\n","Wir benennen bei der Abfrage die Spalten mit der `AS`-Klausel nach den Vorgeben unseres Datenmodells um.\n","\n","Um eindeutige Einträge in der Zieltabelle zu erhalten, nutzen wir die `DISTINCT`-Klausel.\n","\n","Da wir Daten aus zwei Quelltabellen zusammenführen, müssen wir zwei getrennte SQL-Statements nutzen, die wir mit einem `UNION`-Statement verbinden. Die abgefragten Spalten beider Statements müssen dabei identische Namen und Datentypen haben (ggf. müssen hierzu mit `AS` Aliasse definiert werden). `UNION` stellt automatisch sicher, dass Zeilen, die in beiden Teilabfragen identisch sind, zusammengeführt werden (im Sinne eines `DISTINCT` über beide Abfragen zusammen). Wenn Sie stattdessen `UNION ALL` verwenden, bleiben doppelte Einträge erhalten, was aber in einer Dimensionstabelle die Bedingung, dass alle Einträge disjunkt sein müssen, verletzen würde."],"metadata":{"id":"Wrjb1_5xoNF8"}},{"cell_type":"code","source":["%%sql $db_url_staging\n","DELETE FROM reporting.d_aufnahmegrund;\n","\n","INSERT INTO reporting.d_aufnahmegrund (aufnahmegrund_id, aufnahmegrund_name)\n","\n","SELECT DISTINCT\n","       aufnahmegrund      AS aufnahmegrund_id,\n","       aufnahmegrund_text AS aufnahmegrund_name\n","  FROM bielefeld.faelle\n","\n","UNION\n","\n","SELECT DISTINCT\n","       code               AS aufnahmegrund_id,\n","       description        AS aufnahmegrund_name\n","  FROM mannheim.encounters;"],"metadata":{"id":"fJQq8K0--nHe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3. Ergebniskontrolle\n","\n","Anschließend können wir mit einer `SELECT`-Abfrage überprüfen, ob die Tabelle korrekt erstellt wurde.\n"],"metadata":{"id":"kf4Zkcu2qU55"}},{"cell_type":"code","source":["%%sql $db_url_staging\n","SELECT *\n","  FROM reporting.d_aufnahmegrund\n"," LIMIT 10"],"metadata":{"id":"d3REna2T-nNb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Dimensionstabelle D_LOINC\n","\n","Im Folgenden erstellen wir eine etwas komplexere Dimensionstabelle mit zwei Ebenen für die Gliederung der Laborbefunde im Data Warehouse.\n","\n","LOINC (Logical Observation Identifiers Names and Codes) ist eine internationale Terminologie, die zunächst im Laborbereich, inzwischen aber sehr breit für jegliche \"Beobachtungen\" (Observations) durchgesetzt hat. LOINC ist auch die Vorgabe für die semantische Codierung von Laborbefunden in der elektronischen Patientenakte im Deutschen Gesundheitswesen. LOINC ist eine Mehrachsige Terminologie, die neben eindeutigen Bezeichnern für die Observations auch eine Reihe strukturierter Merkmale z.B. zum untersuchten Parameter und der Art der Messung macht. Dies trägt zur Vergleichbarkeit von Befunddaten verschiedener Labore & Standorte bei. Weitere Informationen zu LOINC gibt es auf der [LOINC-Homepage](https://loinc.org/get-started/).\n","\n","Als Rohdaten verwenden wir hier nicht die Quelldaten der beiden Standorte, sondern den im Dimensionsbereich bereitgestellten Auszug der LOINC-Hierarchie, der zusätzliche Attribute enthält."],"metadata":{"id":"ws1ZQGdJ2Vcy"}},{"cell_type":"markdown","source":["### 1. Zieltabelle anlegen\n","\n","Hierzu legen wir zunächst mit einem CREATE-Table die Tabelle in der Reporting-Datenbank (leer) an."],"metadata":{"id":"6Q-J502o2-JE"}},{"cell_type":"code","source":["%%sql $db_url_staging\n","DROP TABLE IF EXISTS reporting.d_loinc;\n","\n","CREATE TABLE reporting.d_loinc (\n","  loinc_klasse_id   VARCHAR(10),\n","  loinc_klasse_name VARCHAR(10),\n","  loinc_id          VARCHAR(15),\n","  loinc_name        VARCHAR(50),\n","  loinc_langname    VARCHAR(200),\n","  loinc_component   VARCHAR(50),\n","  loinc_property    VARCHAR(50),\n","  loinc_timeaspect  VARCHAR(10),\n","  loinc_system      VARCHAR(50),\n","  loinc_scaletype   VARCHAR(10),\n","  loinc_methodtype  VARCHAR(150)\n",");"],"metadata":{"id":"ZNiGHUI-2_WT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2. Daten aus der Quelltabelle auslesen & in Zieltabelle schreiben\n","\n","Die Dimension D_LOINC hat laut DWH-Datenmodell die beiden folgenden Ebenen:\n","* LOINC_KLASSE: übergeordnete Klasse eines Parameters\n","* LOINC: Parameter & dazugehörige Attribute\n","\n","Die LOINC-Klasse ist in den Quelldaten (wie oben bei der Fallart) nur mit einem textuellen Code, jedoch keiner ID abgebildet. Da der Code sprechend ist, tragen wir den Text der Klasse identisch in die _ID- und _NAME-Spalten ein.\n","\n","Da die Quelldaten für beide Ebenen der Hierarchie aus der gleichen Quelltabelle stammen, besteht die Aufgabe hier nur darin, die Spalten gemäß den Vorgaben des DWH-Datenmodells umzubenennen und in die Zieltabelle per `INSERT INTO  ... SELECT ...` auszuliefern."],"metadata":{"id":"sIGTQnme3JKn"}},{"cell_type":"code","source":["%%sql $db_url_staging\n","DELETE FROM reporting.d_loinc;\n","\n","INSERT INTO reporting.d_loinc (loinc_klasse_id, loinc_klasse_name, loinc_id, loinc_name, loinc_langname, loinc_component, loinc_property, loinc_timeaspect, loinc_system, loinc_scaletype, loinc_methodtype)\n","\n","SELECT class            AS loinc_klasse_id,\n","       class            AS loinc_klasse_name,\n","       loinc_num        AS loinc_id,\n","       shortname        AS loinc_name,\n","       long_common_name AS loinc_langname,\n","       component        AS loinc_component,\n","       property         AS loinc_property,\n","       time_aspct       AS loinc_timeaspect,\n","       system           AS loinc_system,\n","       scale_typ        AS loinc_scaletype,\n","       method_typ       AS loinc_methodtype\n","  FROM dimensions.loinc;"],"metadata":{"id":"kgXA8EFu3NcG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3. Ergebniskontrolle\n","\n","Anschließend können wir mit einer `SELECT`-Abfrage überprüfen, ob die Tabelle korrekt erstellt wurde."],"metadata":{"id":"WfABHNED3Rhs"}},{"cell_type":"code","source":["%%sql $db_url_staging\n","SELECT *\n","  FROM reporting.d_loinc\n"," LIMIT 10"],"metadata":{"id":"cqUbW3ef3SpH"},"execution_count":null,"outputs":[]}]}