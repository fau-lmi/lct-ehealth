{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SvxhKbdnO7j3"
   },
   "source": [
    "# 01-01 Extraktion Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbsb3U9UkAl1"
   },
   "source": [
    "## Konfiguration des Notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1D0zfQ-IzSGk"
   },
   "outputs": [],
   "source": [
    "# Ggf. fehlende Pakete installieren\n",
    "!python3 -m pip install --quiet ipython-sql petl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "44KM1u1Zzn3-"
   },
   "outputs": [],
   "source": [
    "import petl as etl\n",
    "import csv\n",
    "import sys\n",
    "%load_ext sql\n",
    "%config SqlMagic.style = '_DEPRECATED_DEFAULT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MP1zDb_s0-rq"
   },
   "outputs": [],
   "source": [
    "# Konfiguration\n",
    "base_url_quellen = \"https://raw.githubusercontent.com/fau-lmi/lct-ehealth/main/07-ETL+DWH/data\"\n",
    "base_url_staging = \"./\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yEl6JfM2mudq"
   },
   "source": [
    "**Hinweise zur Datenbankanbindung**\n",
    "\n",
    "Im folgenden Block werden die Verbindungen zur Datenbank hergestellt. Normalerweise w√ºrden wir f√ºr das Staging eine einzige Zieldatenbank nutzen, in der wir verschiedene Schemata als Namensr√§ume f√ºr die 3 Quellsysteme (Bielefeld, Mannheim, Dimensionen) nutzen w√ºrden. Die Datenbanken werden dabei als `*.sqlite`-Dateien im aktuellen Verzeichnis des Rechners angelegt, auf dem dieses Notebook ausgef√ºhrt wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-vj4aaez2LXG"
   },
   "outputs": [],
   "source": [
    "# Datenbankverbindung als Pfad (f√ºr das ETL) & iPython SQL (f√ºr die Abfragen) herstellen\n",
    "db_path_stg_bielefeld  = base_url_staging + \"stg_bielefeld.sqlite\"\n",
    "db_path_stg_mannheim   = base_url_staging + \"stg_mannheim.sqlite\"\n",
    "db_path_stg_dimensions = base_url_staging + \"stg_dimensions.sqlite\"\n",
    "db_path_staging        = base_url_staging + \"staging.sqlite\"\n",
    "\n",
    "db_url_stg_bielefeld  = \"sqlite:///\" + db_path_stg_bielefeld\n",
    "db_url_stg_mannheim   = \"sqlite:///\" + db_path_stg_mannheim\n",
    "db_url_stg_dimensions = \"sqlite:///\" + db_path_stg_dimensions\n",
    "db_url_staging        = \"sqlite:///\" + db_path_staging\n",
    "\n",
    "%sql $db_url_stg_bielefeld\n",
    "%sql $db_url_stg_mannheim\n",
    "%sql $db_url_stg_dimensions\n",
    "%sql $db_url_staging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v3zUK0IEnDb_"
   },
   "source": [
    "Da SQLite keine Schemata unterst√ºtzt, legen wir stattdessen 3 getrennte Staging-Datenbanken an (stg_bielefeld, stg_mannheim & stg_dimensions). SQLite erlaubt √ºber den `ATTACH DATABASE`-Befehl, diese 3 Datenbanken anschlie√üend von einer gemeinsamen Datenbank (staging) aus zu b√ºndeln und gemeinsam darauf zuzugreifen.\n",
    "\n",
    "Wir k√∂nnen  anschlie√üend von der √ºbergeordneten Staging-Datenbank aus gleichzeitig auf Tabellen jeder der 3 eingeh√§ngten Datenbanken zugreifen, indem wir z.B. `SELECT * FROM stg_bielefeld.faelle` schreiben (l√§dt die Daten der F√§lle-Tabelle der Bielefelder Staging-Datenbank)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qpl48MBD2LfC"
   },
   "outputs": [],
   "source": [
    "%%sql $db_url_staging\n",
    "ATTACH DATABASE :db_path_stg_bielefeld  AS bielefeld;\n",
    "ATTACH DATABASE :db_path_stg_mannheim   AS mannheim;\n",
    "ATTACH DATABASE :db_path_stg_dimensions AS dimensions;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mfEtqN2sne-F"
   },
   "source": [
    "## Rohdatentabelle extrahieren und in Staging-Bereich laden\n",
    "\n",
    "In den folgenden Blocks nutzen wir das PETL-Paket, um mit wenigen Befehlen die Tabelle 'standorte.tsv' aus den Dimensions-Rohdaten in die Staging-Datenbank stg_dimensions zu √ºbertragen.\n",
    "\n",
    "### 1. Zieltabelle anlegen\n",
    "\n",
    "Hierzu legen wir zun√§chst mit einem CREATE-Table die Tabelle in der Staging-Datenbank (leer) an.\n",
    "\n",
    "üí° *Hinweis:* Der Code zur Anlage der Tabelle muss in einer eigenen Zelle stehen, da der `%%sql`-Befehl f√ºr mehrzeilige SQL-Statements nicht gemeinsam mit anderen Befehlen im gleichen Block verarbeitet werden kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-bb8aZtj-nFA"
   },
   "outputs": [],
   "source": [
    "%%sql $db_url_staging\n",
    "DROP TABLE IF EXISTS dimensions.standorte;\n",
    "CREATE TABLE dimensions.standorte (\n",
    "  standort_id   VARCHAR(1),\n",
    "  standort_name VARCHAR(20)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wrjb1_5xoNF8"
   },
   "source": [
    "### 2. Daten aus Quelldatei auslesen & in Zieltabelle schreiben\n",
    "\n",
    "Als n√§chstes laden wir mit der PETL-Funktion `fromtsv()` die Quelldaten aus der Datei 'standorte.tsv' und speichern diese direkt anschlie√üend mit der PETL-Funktion `todb()` in die Zieldatenbank stg_dimensions.\n",
    "\n",
    "Bei der Funktion `fromtsv()` muss im Regelfall nur der Pfad/Dateiname der zu ladenden Datei angegeben werden. Alternativ kann mit `fromcsv()` auch eine Komma-getrennte Datei geladen bzw. mit dem Parameter `delimiter=\";\"` ein anderes Trennzeichen vorgegeben werden.\n",
    "\n",
    "Bei der Funktion `todb()` m√ºssen die geladene Datentabelle, die Datenbankverbindung und der Name der Tabelle in der Zieldatenbank angegeben werden. Optional kann mit dem Parameter `create=True` angegeben werden, dass die Tabelle automatisch in der Zieldatenbank angelegt werden soll. Hierbei werden allerdings (bei zuvor aus TSV/CSV-Dateien geladenen Rohdaten) nicht immer die korrekten Datentypen verwendet. Es ist deshalb sinnvoll, die Tabelle vorher manuell per `CREATE TABLE`-Statement anzulegen. Die Zieltabelle wird von der Funktion zun√§chst geleert und dann mit den Inhalten neu beladen. Wenn die vorhandenen Inhalte bestehen bleiben sollen, muss statt `todb()` die PETL-Funktion `appenddb()` genutzt werden.\n",
    "\n",
    "Dokumentation zu PETL ist hier verf√ºgbar: https://petl.readthedocs.io/en/stable/io.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fJQq8K0--nHe"
   },
   "outputs": [],
   "source": [
    "table_data = etl.fromtsv(base_url_quellen + \"/dimensionen/standorte.tsv\")\n",
    "etl.todb(table_data, db_path_stg_dimensions, 'standorte')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kf4Zkcu2qU55"
   },
   "source": [
    "### 3. Ergebniskontrolle\n",
    "\n",
    "Anschlie√üend k√∂nnen wir mit einer `SELECT`-Abfrage √ºberpr√ºfen, ob die Tabelle korrekt √ºbertragen wurde.\n",
    "\n",
    "üí° *Tipp:* Wenn Sie mit einer `SELECT * FROM tabelle`-Abfrage pr√ºfen wollen, ob Inhalte vorhanden sind, k√∂nnen ggf. sehr gro√üe Datenmengen in das Colab-Notebook geladen werden, was die Laufzeitumgebung bzw. den Browser zum Absturz bringen kann. Erg√§nzen Sie deshalb immer eine `LIMIT <Zahl>`-Klausel am Ende der Abfrage, so dass nur eine feste Zahl von Datens√§tzen geladen wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d3REna2T-nNb"
   },
   "outputs": [],
   "source": [
    "%%sql $db_url_staging\n",
    "SELECT *\n",
    "  FROM dimensions.standorte\n",
    " LIMIT 10"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPMH7zwaWjxVDEBTUoXEPHX",
   "provenance": [
    {
     "file_id": "1BL6fz2-jOCEmlbHL_LUVxhtHzK709prp",
     "timestamp": 1714904625208
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
