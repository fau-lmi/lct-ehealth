{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SvxhKbdnO7j3"
   },
   "source": [
    "# 03-01 Pandas-Demo\n",
    "\n",
    "## Hinweise zur Übung\n",
    "\n",
    "Diese Demo schließt an die vorherige SQL-Übung an und zeigt, wie die wesentlichen Abfragemöglichkeiten statt auf eine SQL-Dataenbank auch in-memory mit Python-Dataframes und Funktionen des Pandas-Package umgesetzt werden können.\n",
    "\n",
    "Caveat: Dataframes befinden sich komplett im Speicher (\"in-memory\"). Nachdem sie geladen sind, können Abfragen deshalb extrem schnell (ggf. auch schneller als auf einer Datenbank) durchgeführt werden. Allerdings können die Quelldataen ggf. größer als der verfügbare Speicher sein. In diesem Fall ist es sinnvoller, Vorselektionen auf Ebene der Datenbank durchzuführen und nur die tatsächlich benötigten Daten zu laden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbsb3U9UkAl1"
   },
   "source": [
    "## Konfiguration des Notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1D0zfQ-IzSGk"
   },
   "outputs": [],
   "source": [
    "# Ggf. fehlende Pakete installieren\n",
    "!pip install --quiet ipython-sql pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "44KM1u1Zzn3-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import urllib.request\n",
    "import gzip\n",
    "import shutil\n",
    "import pandas\n",
    "%load_ext sql\n",
    "%config SqlMagic.style = '_DEPRECATED_DEFAULT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MP1zDb_s0-rq"
   },
   "outputs": [],
   "source": [
    "# Konfiguration\n",
    "base_url_quellen   = \"https://raw.githubusercontent.com/fau-lmi/lct-ehealth/main/08-Datenanalyse+Visualisierung/data\"\n",
    "base_url_reporting = \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4IYsvJsGthEU"
   },
   "outputs": [],
   "source": [
    "# SQlite-Datenbanken aus Github auf den Jupyter-Server herunterladen\n",
    "urllib.request.urlretrieve(base_url_quellen + \"/dwh/reporting.sqlite.gz\", base_url_reporting + \"reporting.sqlite.gz\")\n",
    "\n",
    "# Die Sqlite-Datenbank ist aufgrund ihrer Größe gezipped und muss vor der Nutzung noch entpackt werden\n",
    "with gzip.open(base_url_reporting + \"reporting.sqlite.gz\", \"rb\") as f_in:\n",
    "    with open(base_url_reporting + \"reporting.sqlite\", \"wb\") as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p_O_S1FO6crA"
   },
   "outputs": [],
   "source": [
    "# Datenbankverbindung als Pfad (für das ETL) & iPython SQL (für die Abfragen) herstellen\n",
    "db_path_reporting      = base_url_reporting + \"reporting.sqlite\"\n",
    "\n",
    "db_url_reporting      = \"sqlite:///\" + db_path_reporting\n",
    "\n",
    "%sql $db_url_reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2hk6avHq7J62"
   },
   "source": [
    "## Laden von Daten aus der SQLite-Datenbank in Dataframes\n",
    "\n",
    "Auch in diesem Abschnitt nutzen wir die SQLite-Datenbank mit dem Kurs-DWH. Wir laden die jeweils benötigten Daten über die bekannten SQL-Abfragen und überführen sie in einen Dataframe.\n",
    "\n",
    "Bei Abfragen mit dem `%sql`-Kommando haben wir das Ergebnis bisher nur angezeigt. Wir ändern das hier, indem wir das Ergebnis in eine Variable speichern und anschließend mit der Methode `DataFrame()` in einen Dataframe konvertieren.\n",
    "\n",
    "Die Überführung der Abfrageergebnisse in eine Variable kann nur bei \"einzeiligen\" SQL-Statements genutzt werden (`%sql`), nicht jedoch bei mehrzeiligen (`%%sql`). Wir schreiben das SQL-Statement daher zunächst in eine Variable und übergeben die Variable in ein einzeliges `%sql`-Statement. Die 3-fachen Anführungszeichen `\"\"\"` dienen in Python dazu, längere Texte mit Zeilenumbrüchen in einem Rutsch in eine Variable zu schreiben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R7Gb75Sb6smZ"
   },
   "outputs": [],
   "source": [
    "# SQL-Statement zur Abfrage der kompletten Tabelle D_PATIENT in Variable speichern\n",
    "sql = \"\"\"\n",
    "SELECT *\n",
    "  FROM d_patient\n",
    "\"\"\"\n",
    "\n",
    "# Abfrage ausführen und Ergebnis in Variable resultset speichern\n",
    "resultset = %sql $db_url_reporting $sql\n",
    "\n",
    "# Resultset in Dataframe überführen\n",
    "df_patient = resultset.DataFrame()\n",
    "\n",
    "# Erste Zeilen des Dataframe ausgeben\n",
    "df_patient.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KyuexSOSQMmE"
   },
   "outputs": [],
   "source": [
    "# Abfrage der Tabelle F_FAELLE und Ablage in Dataframe df_faelle\n",
    "sql = \"\"\"\n",
    "SELECT *\n",
    "  FROM f_faelle\n",
    "\"\"\"\n",
    "resultset = %sql $db_url_reporting $sql\n",
    "df_faelle = resultset.DataFrame()\n",
    "df_faelle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NTesd0NpOtl6"
   },
   "outputs": [],
   "source": [
    "# Abfrage der Tabelle D_FALLART und Ablage in Dataframe df_fallart\n",
    "sql = \"\"\"\n",
    "SELECT *\n",
    "  FROM d_fallart\n",
    "\"\"\"\n",
    "resultset = %sql $db_url_reporting $sql\n",
    "df_fallart = resultset.DataFrame()\n",
    "df_fallart.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aZ59qDAMPf4x"
   },
   "outputs": [],
   "source": [
    "# Abfrage der Tabelle D_DIAGNOSE und Ablage in Dataframe df_diagnose\n",
    "sql = \"\"\"\n",
    "SELECT *\n",
    "  FROM d_diagnose\n",
    "\"\"\"\n",
    "resultset = %sql $db_url_reporting $sql\n",
    "df_diagnose = resultset.DataFrame()\n",
    "df_diagnose.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l7FU7pqDSuPO"
   },
   "source": [
    "## Einfache Abfragen auf Datafames\n",
    "\n",
    "Dataframes können wie eine Datenbanktabelle mit verschiedenen Pandas-Methoden abgefragt werden, um z.B. nur bestimmte Spalten auszuwählen oder Zeilen nach verschiedenen Kriterien zu filtern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4mCqWSSTln1"
   },
   "source": [
    "### Spalten eines Dataframe selektieren\n",
    "\n",
    "Die Spalten eines Dataframes können über die aus der Datenbank übernommenen Spaltennamen adressiert werden (entsprechend der `SELECT`-Klausel eines SQL-Statements). Die Notation ist hierbei `dataframename[['Spalte1'], ['Spalte2'], ...]`. Mit der `head()`-Methode wird wieder nur der Anfang des Ergebnissatzes ausgegeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vqSP-kEdTl0G"
   },
   "outputs": [],
   "source": [
    "# Nur die Spalten patient_id & patient_nachname ausgeben\n",
    "df_patient[[\"patient_id\", \"patient_nachname\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "097WP1KOUSQ3"
   },
   "source": [
    "### Zeilen eines Dataframe selektieren\n",
    "\n",
    "Vergleichbar zur `WHERE`-Klausel eines SQL-Statements kann in Pandas die `query()`-Methode genutzt werden, um ein oder mehrere Kriterien zur Selektion von Zeilen eines Dataframe anzuwenden. Spalten des Dataframe können dabei über ihren Namen direkt angesprochen werden.\n",
    "\n",
    "In den Query-Kriterien können auch Variablen und (ausgewählte) Funktionen benutzt werden. Z.B. können Teilstringvergleiche u.a. durch Anfügen von `.str.startswith('text')` oder `.str.contains('')` durchgeführt werden.\n",
    "\n",
    "Mehrere Kriterien können mit Bool'schen Operatoren verbunden werden, und zwar sowohl als Textkommandos (and, or, not) als auch Symbole (&, |).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fm9pK_P9USXs"
   },
   "outputs": [],
   "source": [
    "# Filterung auf männliche Patienten\n",
    "df_patient.query(\"patient_geschlecht == 'M'\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w20wwzAG5_sg"
   },
   "outputs": [],
   "source": [
    "# Patienten selektieren, deren Geburtsdatum (über einen Stringvergleich) mit dem Jahr 1984 beginnt\n",
    "df_patient.query(\"patient_gebdat.str.startswith('1984')\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9kbe35ASC_7m"
   },
   "outputs": [],
   "source": [
    "# Männliche Patienten selektieren, deren Geburtsdatum (über einen Stringvergleich) mit dem Jahr 1984 beginnt\n",
    "df_patient.query(\"patient_geschlecht == 'M' and patient_gebdat.str.startswith('1984')\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UMFJLrbETVWl"
   },
   "source": [
    "### Eindeutige Datensätze auslesen\n",
    "\n",
    "Analog zum `DISTINCT`-Keyword aus SQL gibt es in Pandas die `drop_duplicates()`-Methode, um über alle enthaltenen Spalten eindeutige Datensätze auszugeben.\n",
    "\n",
    "Pandas erlaubt es, Methodenaufrufe hintereinander zu schreiben, so dass sie im Sinne einer Pipeline nacheinander ausgeführt werden (hier: erst `drop_duplicates()` und danach `head()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y61MpMGTE5oA"
   },
   "outputs": [],
   "source": [
    "# Nur die eindeutigen Ausprägungen der Spalte patient_geschlecht ausgeben\n",
    "df_patient[['patient_geschlecht']].drop_duplicates().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P74YTT46bICQ"
   },
   "source": [
    "## Gruppieren und aggregieren von Tabelleninhalten\n",
    "\n",
    "Analog zu `GROUP BY`und den verschiedenen Aggregatfunktionen in SQL stellt Pandas die `groupby()`-Methode sowie die `agg()`-Methode bereit. Die Funktionen können wie in den vorherigen Beispielen als Pipeline hintereinander verkettet werden.\n",
    "\n",
    "In der `groupby()`-Methode kann eine einzelne Spalte in Anführungsstrichen als Argument eingetragen werden (z.B. `groupby('Spalte')`, während mehrere Spalten als Array in eckige Klammern geschrieben werden müssen (z.B. `groupby(['Spalte1', 'Spalte2'])`).\n",
    "\n",
    "In der `agg()`-Methode können die Aggregationen auf verschiedene Weisen angegeben werden. Die folgende Form erlaubt es, für mehrere Spalten verschiedene Aggregationen anzugeben und den daraus resultierenden Spalten explizite Namen zuzuweisen: `agg(aggregierte_spalte1=(\"quellspalte1\", \"aggregatfunktion\"), aggregierte_spalte2=(\"quellspalte2\", \"aggregatfunktion\"))`.\n",
    "\n",
    "Beispiel:\n",
    "* `dataframe.agg(erloes_sum=(\"erloes\", \"sum\"), erloes_avg=(\"erloes\", \"mean\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8JCsab0aUBTD"
   },
   "source": [
    "### Datensätze aggregieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b1F3bhOgcHra"
   },
   "outputs": [],
   "source": [
    "# Patiententabelle nach Geschlecht gruppieren und die Anzahl der Datensätze zählen\n",
    "df_patient.groupby(\"patient_geschlecht\").agg(n=(\"patient_geschlecht\", \"size\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M7k9Y_W6UZWB"
   },
   "outputs": [],
   "source": [
    "# Fälletabelle nach Fallart gruppieren und die Fallzahl, den Mittelwert der Liegedauer sowie Summe der Erlöse aggregieren\n",
    "df_faelle.groupby(\"fallart_id\").agg(fallzahl=(\"fallart_id\", \"size\"), liegedauer_avg=(\"liegedauer_tage\", \"mean\"), erloes_sum=(\"erloes_fallpauschale\", \"sum\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q03tszPfeKYW"
   },
   "source": [
    "### Filterung bei Aggregatfunktionen\n",
    "\n",
    "Zur `HAVING`-Klausel aus SQL gibt es kein eigenes Äquivalent, stattdessen kann einfach die oben beschriebene `query()`-Methode in der Pipeline hinter die Aggregation gestellt werden.\n",
    "\n",
    "Da die Pipelines bei vielen Verarbeitungsschritten unübersichtlich werden können, ist es möglich, die Schritte auf mehrere Zeilen aufzuteilen. Ans Ende der jeweils vorherigen Zeile muss ein Backslash (`\\`) gestellt werden und die fortsetzenden Zeilen müssen eingerückt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3P_oMBs5dZWS"
   },
   "outputs": [],
   "source": [
    "# Fälletabelle nach Fallart gruppieren und die Fallzahl, den Mittelwert der Liegedauer sowie Summe der Erlöse aggregieren\n",
    "# - Filterung nach der Aggregation auf Einträge mit mittlerer Liegedauer > 6 Tage\n",
    "df_faelle.groupby(\"fallart_id\") \\\n",
    "  .agg(fallzahl=(\"fallart_id\", \"size\"), liegedauer_avg=(\"liegedauer_tage\", \"mean\"), erloes_sum=(\"erloes_fallpauschale\", \"sum\")) \\\n",
    "  .query(\"liegedauer_avg > 6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3DfPcwkUrR8"
   },
   "source": [
    "## Zusammenführen mehrerer Dataframes\n",
    "\n",
    "Das Gegenstück zur `JOIN`-Klausel in SQL ist die `merge()`-Methode in Pandas, die ebenfalls inner sowie left/right outer Joins über eine oder mehrere Spalten der einbezogenen Dataframes erlaubt.\n",
    "\n",
    "Die `merge()`-Methode kann in einer Pipeline an den ersten (\"linken\") in den Join einbezogenen Dataframe angehängt werden und enthält als erstes Argument den \"rechts\" zu verbindenen Dataframe: `dataframe1.merge(dataframe2)`.\n",
    "\n",
    "Mit dem `on´-Argument werden die Spalten festgelegt, die für die Verknüpfung der Dataframes eingesetzt werden sollen:\n",
    "* `on='Spalte1'`: es soll nur über Spalte1 gejoined werden, die in beiden Dataframes identisch benannt ist\n",
    "* `on=['Spalte1', 'Spalte2']`: es soll über Spalte1 und Spalte2 gejoined werden, die in beiden Dataframes identisch benannt sind\n",
    "* `left_on=['LinkeSpalte1', 'LinkeSpalte2'], right_on=[: 'RechteSpalte1', 'RechteSpalte2']`: wenn die Spalten der Join-Kriterien in den beiden Dataframes unterschiedlich benannt sind, müssen sie separat in `left_on`und `right_on`-Argumenten in identischer Reihenfolge angegeben werden.\n",
    "\n",
    "Identisch benannte Spalten, die als Join-Kriterien angegeben wurden, werden im Ergebnisdataframe zusammengeführt, während unterschiedlich benannte Spalten übernommen werden. Identisch benannte Spalten außerhalb der Join-Kriterien werden separat in den Ergebnisdataframe benommen und ggf. mit einem Suffix qualifiziert.\n",
    "\n",
    "\"Überflüssige\" Spalten, die z.B. wegen Redundanzen zwischen den Tabellen im Ergebnisdataframe enthalten sind, können mit der `drop()`-Methode entfernt werden.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "09uLxupAYB7o"
   },
   "source": [
    "### INNER JOIN: Schnittmenge von Dataframes bilden\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f9ZbXQOnUsaa"
   },
   "outputs": [],
   "source": [
    "# Spalten für Fall-ID und Fallart-ID aus dem Dataframe df_faelle selektieren und\n",
    "# den Fallartbezeichner per merge() hinzufügen\n",
    "df_faelle[[\"fall_id\", \"fallart_id\"]].merge(df_fallart, on=\"fallart_id\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ktPxatraTpcW"
   },
   "outputs": [],
   "source": [
    "# Diagnosebezeichner zu den Fällen ergänzen\n",
    "df_faelle[[\"fall_id\", \"hauptdiagnose_snomed_id\"]] \\\n",
    "  .merge(df_diagnose[[\"snomed_id\", \"snomed_name\"]], left_on=\"hauptdiagnose_snomed_id\", right_on=\"snomed_id\") \\\n",
    "  .head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dNWOWn_WPpgT"
   },
   "outputs": [],
   "source": [
    "# Pipeline für Aggregation & Zählung der Fälle nach Fallart und Ergänzung der Fallartbezeichner\n",
    "df_faelle.merge(df_fallart, on=\"fallart_id\") \\\n",
    "  .groupby([\"fallart_id\", \"fallart_name\"]) \\\n",
    "  .agg({\"fallart_id\": \"size\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9zhGREozZpJG"
   },
   "source": [
    "### OUTER JOINs: Datensätze einbeziehen, für die es keine Entsprechung in beiden Dataframes gibt\n",
    "\n",
    "Mit dem `how`-Argument kann festgelegt werden, ob ein inner (default), left oder right outer join verwendet werden soll."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "90HFlF3AZpTX"
   },
   "outputs": [],
   "source": [
    "# Fälle mit Bezeichnern für ihre Hauptdiagnosen ausgeben, aber Fälle ohne Hauptdiagnose beibehalten\n",
    "df_faelle[[\"fall_id\", \"hauptdiagnose_snomed_id\"]] \\\n",
    "  .merge(df_diagnose[[\"snomed_id\", \"snomed_name\"]], left_on=\"hauptdiagnose_snomed_id\", right_on=\"snomed_id\", how=\"left\") \\\n",
    "  .drop(columns=\"snomed_id\") \\\n",
    "  .head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mtaLmqptVjej"
   },
   "source": [
    "### Mehrere  Dataframes \"hintereinander\" zusammenfügen\n",
    "\n",
    "Neben dem \"horizontalen\" Verknüpfen von Dataframes mit der `merge()`-Methode können Dataframes auch mit der `concat()`-Funktion \"hintereinander gestellt\" werden. Die Spalten beides Datensätze müssen dafür identische Namen und Spalten haben.\n",
    "\n",
    "Im Gegensatz zu den oben beschriebenen Methoden ist `concat()` eine eigenständige Funktion, der beide (oder auch mehrere) hintereinanderzusetzende Dataframes als Argument übergeben werden müssen, und keine Methode eines einzelnen Dataframe.\n",
    "\n",
    "Im Gegensatz zum `UNION`-Statement in SQL werden hier beide Datensätze vollständig hintereinander gesetzt, unabhängig davon, ob es doppelte Datensätze gibt. Die `concat()`-Methode enspricht also einem `UNION ALL`-Statement in SQL. Falls eindeutige Datensätze benötigt werden, können mit der `drop_duplicates()`-Methode Zeilen mit identischen Inhalten zusammengeführt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TCgaGthWVkBj"
   },
   "outputs": [],
   "source": [
    "# Zwei Datensätze mit Teilen des Diagnosehierarchie selektieren (Bronchitis und Diabetes)\n",
    "df_bronchitis = df_diagnose.query(\"snomed_name.str.contains('bronchitis')\")\n",
    "df_diabetes   = df_diagnose.query(\"snomed_name.str.contains('diabetes')\")\n",
    "\n",
    "# Beide Datensätze mit concat() zusammenfügen\n",
    "pandas.concat([df_bronchitis, df_diabetes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5fphFVoDsxpb"
   },
   "source": [
    "## Dataframes pivotieren: \"Rechteck- & Schlauchtabellen\"\n",
    "\n",
    "Datentabellen sind häufig als \"Rechtecke im Querformat\"\n",
    "(\"wide format\") angelegt: ein Datensatz besteht aus wenigen identifizierenden oder charakterisierenden Merkmalen auf der linken Seite (vgl. *Dimensionen* im Data Warehouse) und vielen zugehörigen Kennzahlen auf der rechten Seite (vgl. *Fakten* im Data Warehouse). Vorteil dieser Darstellungsform ist, dass alle Merkmale zu einer Beobachtungseinheit (z.B. Behandlungsfall) in einer Zeile beieinander stehen, und dass alle Spalten explizit benannt sind und individuelle zu ihrem Inhalt passende Datentypen haben. Hieraus ergeben sich aber auch Nachteile:\n",
    "* die Struktur ist fest vorgegeben und kann daher nur schlecht mit 1:n-zugeordneten Merkmalen umgehen (z.B. beliebige Anzahl von Diagnosen eines Behandlungsfalls)\n",
    "* Programme, die auf diese Datenstruktur zugreifen, müssen die Spaltenbezeichner explizit kennen. Änderungen der Tabellenstruktur erfordern dann auch Anpassungen des Programms\n",
    "\n",
    "Alternativ zu diesen \"Rechtecktabellen\" haben sich \"Schlauchtabellen\" etabliert, die quasi \"hochkant\" (\"long format\") stehen und nur aus den identifizierenden/charakterisierenden Merkmalen, einem Feld für die Bezeichnung des Merkmals und einem weiteren für dessen Ausprägung bestehen. Diese Struktur ist generisch und kann ohne Änderung der Tabelle oder des darauf zugreifenden Programms auch mit wechselnden Inhalten umgehen. Konkret kann das z.B. die Visualisierung mehrerer Kennzahlen in einem gemeinsamen Diagramm vereinfachen. Außerdem können wechselnde Kardinalitäten leicht abgebildet werden (z.B. 2 Diagnosen bei Behandlungsfall A, 20 Diagnosen bei Fall B). Auch hier ergeben sich jedoch Nachteile:\n",
    "* da es nur eine Spalte für die Ausprägung der Fakten gibt, teilen sich diese einen gemeinsamen Datentyp; wenn die Fakten verschiedene Typen benötigen, muss man hier ggf. als \"kleinsten gemeinsamen Nenner\" ein Textfeld nutzen und Daten anschließend zurückkonvertieren.\n",
    "* die Tabelle kann je nach Anzahl der Attribute sehr lang und unübersichtlich werden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M5db0zMoy-aB"
   },
   "source": [
    "### Dataframe vom \"wide\"- ins \"long\"-Format transformieren\n",
    "\n",
    "In Pandas kann mit der Methode `melt` vom *wide*- ins *long*-Format pivotiert werden. Folgende Parameter sind dazu in der Regel anzugeben:\n",
    "* `id_vars`: Array mit den identifizierenden bzw. charakterisierenden Spalen (\"linker Teil\" der Tabelle)\n",
    "* `value_vars`: Optionales Array mit den Spalten, aus denen die Kennzahlen übernommen werden sollen; wenn es nicht angegeben wird, übernimmt `melt()` defaultmäßig alle Spalten, die nicht in `id_vars` als identifizierend angegeben wurden.* `var_name`: Name der Spalte, die die Attribute kennzeichnen soll; sie wird mit den Spaltennamen befüllt, aus denen die Kennzahlen kommen\n",
    "* `value_name`: Name der Spalte, in die die Kennzahlen übertragen werden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P5YDQY0kv25X"
   },
   "outputs": [],
   "source": [
    "# Identifizierende Spalte fall_id sowie die Kennzahlen Aufnahmealte, Liegedauer und Erlös selektieren,\n",
    "# die Zeilen auf 3 Bielefelder Fälle einschränken\n",
    "# und die Tabelle mit der melt()-Methode vom \"wide\"- ins \"long\"-Format pivotieren.\n",
    "df_faelle_long = df_faelle[[\"fall_id\", \"aufnahmealter_jahre\", \"liegedauer_tage\", \"erloes_fallpauschale\"]] \\\n",
    "  .query(\"fall_id in ['B-1', 'B-2', 'B-3']\") \\\n",
    "  .melt(id_vars=[\"fall_id\"], var_name=\"kennzahl\", value_name=\"wert\") \\\n",
    "  .sort_values(by=\"fall_id\")\n",
    "df_faelle_long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s0IirNSszHWN"
   },
   "source": [
    "### Dataframes vom \"long\"- ins \"wide\"-Format transformieren\n",
    "\n",
    "Mit der Pandas-Funktion `pivot()` kann eine \"hochkant\" strukturierte Tabelle im \"long\"-Format in das klassische \"wide\"-Format gekippt werden.\n",
    "\n",
    "Folgende Parameter sind hierzu in der Regel anzugeben:\n",
    "* `index`: Spalte(n), die als identifizierende/charakterisierende Spalten den linken Teil der Rechtecktabelle ausmachen (sie werden praktisch 1:1 aus der \"long\"-Tabelle übernommen)\n",
    "* `columns`: Spalte der \"long\"-Tabelle, aus der die Spaltennamen für die Fakten der \"wide\"-Tabelle geholt werden sollen (für den rechten Teil)\n",
    "* `values`: Spalte der \"long\"-Tabelle, aus der die Inhalte der Kennzahlen-Spalten im rechten teil geholt werden sollen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SKQD-aPAy8wf"
   },
   "outputs": [],
   "source": [
    "# \"Long\"-Tabelle aus dem vorherigen Abschnitt wieder in ein \"wide\"-Format umklappen:\n",
    "# die Spalte \"fall_id\" ist weiterhin die identifizierende Spalte\n",
    "# die zuvor für die Bezeichner & Inhalte angegebenen Spalten werden auch für die Transformation zurück genutzt\n",
    "df_faelle_long.pivot(index=\"fall_id\", columns=\"kennzahl\", values=\"wert\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOBkOfn8M6eQK2gkcmuqldJ",
   "provenance": [
    {
     "file_id": "1BKCcgS8zssyHgOmY-p6uD5JtTzI63Jlb",
     "timestamp": 1715515667455
    },
    {
     "file_id": "https://github.com/fau-lmi/lct-ehealth-datawarehouse/blob/main/01-01-Extraktion-Demo.ipynb",
     "timestamp": 1714911186628
    },
    {
     "file_id": "1BL6fz2-jOCEmlbHL_LUVxhtHzK709prp",
     "timestamp": 1714904625208
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
