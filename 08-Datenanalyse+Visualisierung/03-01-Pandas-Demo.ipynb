{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1BKCcgS8zssyHgOmY-p6uD5JtTzI63Jlb","timestamp":1715515667455},{"file_id":"https://github.com/fau-lmi/lct-ehealth-datawarehouse/blob/main/01-01-Extraktion-Demo.ipynb","timestamp":1714911186628},{"file_id":"1BL6fz2-jOCEmlbHL_LUVxhtHzK709prp","timestamp":1714904625208}],"toc_visible":true,"authorship_tag":"ABX9TyMZcTyxyDf1dF+wFZT+9EB8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 03-01 Pandas-Demo\n","\n","## Hinweise zur Übung\n","\n","Diese Demo schließt an die vorherige SQL-Übung an und zeigt, wie die wesentlichen Abfragemöglichkeiten statt auf eine SQL-Dataenbank auch in-memory mit Python-Dataframes und Funktionen des Pandas-Package umgesetzt werden können.\n","\n","Caveat: Dataframes befinden sich komplett im Speicher (\"in-memory\"). Nachdem sie geladen sind, können Abfragen deshalb extrem schnell (ggf. auch schneller als auf einer Datenbank) durchgeführt werden. Allerdings können die Quelldataen ggf. größer als der verfügbare Speicher sein. In diesem Fall ist es sinnvoller, Vorselektionen auf Ebene der Datenbank durchzuführen und nur die tatsächlich benötigten Daten zu laden."],"metadata":{"id":"SvxhKbdnO7j3"}},{"cell_type":"markdown","source":["## Konfiguration des Notebooks"],"metadata":{"id":"bbsb3U9UkAl1"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"1D0zfQ-IzSGk"},"outputs":[],"source":["# Ggf. fehlende Pakete installieren\n","!pip install --quiet ipython-sql pandas"]},{"cell_type":"code","source":["import os\n","import sys\n","import urllib.request\n","import gzip\n","import shutil\n","import pandas\n","%load_ext sql"],"metadata":{"id":"44KM1u1Zzn3-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Konfiguration\n","base_url_quellen   = \"https://raw.githubusercontent.com/fau-lmi/lct-ehealth/main/08-Datenanalyse+Visualisierung/data\"\n","base_url_reporting = \"./\""],"metadata":{"id":"MP1zDb_s0-rq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# SQlite-Datenbanken aus Github auf den Jupyter-Server herunterladen\n","urllib.request.urlretrieve(base_url_quellen + \"/dwh/reporting.sqlite.gz\", base_url_reporting + \"reporting.sqlite.gz\")\n","\n","# Die Sqlite-Datenbank ist aufgrund ihrer Größe gezipped und muss vor der Nutzung noch entpackt werden\n","with gzip.open(base_url_reporting + \"reporting.sqlite.gz\", \"rb\") as f_in:\n","    with open(base_url_reporting + \"reporting.sqlite\", \"wb\") as f_out:\n","        shutil.copyfileobj(f_in, f_out)"],"metadata":{"id":"4IYsvJsGthEU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Datenbankverbindung als Pfad (für das ETL) & iPython SQL (für die Abfragen) herstellen\n","db_path_reporting      = base_url_reporting + \"reporting.sqlite\"\n","\n","db_url_reporting      = \"sqlite:///\" + db_path_reporting\n","\n","%sql $db_url_reporting"],"metadata":{"id":"p_O_S1FO6crA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Laden von Daten aus der SQLite-Datenbank in Dataframes\n","\n","Auch in diesem Abschnitt nutzen wir die SQLite-Datenbank mit dem Kurs-DWH. Wir laden die jeweils benötigten Daten über die bekannten SQL-Abfragen und überführen sie in einen Dataframe.\n","\n","Bei Abfragen mit dem `%sql`-Kommando haben wir das Ergebnis bisher nur angezeigt. Wir ändern das hier, indem wir das Ergebnis in eine Variable speichern und anschließend mit der Methode `DataFrame()` in einen Dataframe konvertieren.\n","\n","Die Überführung der Abfrageergebnisse in eine Variable kann nur bei \"einzeiligen\" SQL-Statements genutzt werden (`%sql`), nicht jedoch bei mehrzeiligen (`%%sql`). Wir schreiben das SQL-Statement daher zunächst in eine Variable und übergeben die Variable in ein einzeliges `%sql`-Statement. Die 3-fachen Anführungszeichen `\"\"\"` dienen in Python dazu, längere Texte mit Zeilenumbrüchen in einem Rutsch in eine Variable zu schreiben."],"metadata":{"id":"2hk6avHq7J62"}},{"cell_type":"code","source":["# SQL-Statement zur Abfrage der kompletten Tabelle D_PATIENT in Variable speichern\n","sql = \"\"\"\n","SELECT *\n","  FROM d_patient\n","\"\"\"\n","\n","# Abfrage ausführen und Ergebnis in Variable resultset speichern\n","resultset = %sql $db_url_reporting $sql\n","\n","# Resultset in Dataframe überführen\n","df_patient = resultset.DataFrame()\n","\n","# Erste Zeilen des Dataframe ausgeben\n","df_patient.head()"],"metadata":{"id":"R7Gb75Sb6smZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Abfrage der Tabelle F_FAELLE und Ablage in Dataframe df_faelle\n","sql = \"\"\"\n","SELECT *\n","  FROM f_faelle\n","\"\"\"\n","resultset = %sql $db_url_reporting $sql\n","df_faelle = resultset.DataFrame()\n","df_faelle.head()"],"metadata":{"id":"KyuexSOSQMmE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Abfrage der Tabelle D_FALLART und Ablage in Dataframe df_fallart\n","sql = \"\"\"\n","SELECT *\n","  FROM d_fallart\n","\"\"\"\n","resultset = %sql $db_url_reporting $sql\n","df_fallart = resultset.DataFrame()\n","df_fallart.head()"],"metadata":{"id":"NTesd0NpOtl6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Abfrage der Tabelle D_DIAGNOSE und Ablage in Dataframe df_diagnose\n","sql = \"\"\"\n","SELECT *\n","  FROM d_diagnose\n","\"\"\"\n","resultset = %sql $db_url_reporting $sql\n","df_diagnose = resultset.DataFrame()\n","df_diagnose.head()"],"metadata":{"id":"aZ59qDAMPf4x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Einfache Abfragen auf Datafames\n","\n","Dataframes können wie eine Datenbanktabelle mit verschiedenen Pandas-Methoden abgefragt werden, um z.B. nur bestimmte Spalten auszuwählen oder Zeilen nach verschiedenen Kriterien zu filtern."],"metadata":{"id":"l7FU7pqDSuPO"}},{"cell_type":"markdown","source":["### Spalten eines Dataframe selektieren\n","\n","Die Spalten eines Dataframes können über die aus der Datenbank übernommenen Spaltennamen adressiert werden (entsprechend der `SELECT`-Klausel eines SQL-Statements). Die Notation ist hierbei `dataframename[['Spalte1'], ['Spalte2'], ...]`. Mit der `head()`-Methode wird wieder nur der Anfang des Ergebnissatzes ausgegeben."],"metadata":{"id":"l4mCqWSSTln1"}},{"cell_type":"code","source":["# Nur die Spalten patient_id & patient_nachname ausgeben\n","df_patient[[\"patient_id\", \"patient_nachname\"]].head()"],"metadata":{"id":"vqSP-kEdTl0G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Zeilen eines Dataframe selektieren\n","\n","Vergleichbar zur `WHERE`-Klausel eines SQL-Statements kann in Pandas die `query()`-Methode genutzt werden, um ein oder mehrere Kriterien zur Selektion von Zeilen eines Dataframe anzuwenden. Spalten des Dataframe können dabei über ihren Namen direkt angesprochen werden.\n","\n","In den Query-Kriterien können auch Variablen und (ausgewählte) Funktionen benutzt werden. Z.B. können Teilstringvergleiche u.a. durch Anfügen von `.str.startswith('text')` oder `.str.contains('')` durchgeführt werden.\n","\n","Mehrere Kriterien können mit Bool'schen Operatoren verbunden werden, und zwar sowohl als Textkommandos (and, or, not) als auch Symbole (&, |).\n"],"metadata":{"id":"097WP1KOUSQ3"}},{"cell_type":"code","source":["# Filterung auf männliche Patienten\n","df_patient.query(\"patient_geschlecht == 'M'\").head()"],"metadata":{"id":"fm9pK_P9USXs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Patienten selektieren, deren Geburtsdatum (über einen Stringvergleich) mit dem Jahr 1984 beginnt\n","df_patient.query(\"patient_gebdat.str.startswith('1984')\").head()"],"metadata":{"id":"w20wwzAG5_sg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Männliche Patienten selektieren, deren Geburtsdatum (über einen Stringvergleich) mit dem Jahr 1984 beginnt\n","df_patient.query(\"patient_geschlecht == 'M' and patient_gebdat.str.startswith('1984')\").head()"],"metadata":{"id":"9kbe35ASC_7m"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Eindeutige Datensätze auslesen\n","\n","Analog zum `DISTINCT`-Keyword aus SQL gibt es in Pandas die `drop_duplicates()`-Methode, um über alle enthaltenen Spalten eindeutige Datensätze auszugeben.\n","\n","Pandas erlaubt es, Methodenaufrufe hintereinander zu schreiben, so dass sie im Sinne einer Pipeline nacheinander ausgeführt werden (hier: erst `drop_duplicates()` und danach `head()`)."],"metadata":{"id":"UMFJLrbETVWl"}},{"cell_type":"code","source":["# Nur die eindeutigen Ausprägungen der Spalte patient_geschlecht ausgeben\n","df_patient[['patient_geschlecht']].drop_duplicates().head()"],"metadata":{"id":"y61MpMGTE5oA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Gruppieren und aggregieren von Tabelleninhalten\n","\n","Analog zu `GROUP BY`und den verschiedenen Aggregatfunktionen in SQL stellt Pandas die `groupby()`-Methode sowie die `agg()`-Methode bereit. Die Funktionen können wie in den vorherigen Beispielen als Pipeline hintereinander verkettet werden.\n","\n","In der `groupby()`-Methode kann eine einzelne Spalte in Anführungsstrichen als Argument eingetragen werden (z.B. `groupby('Spalte')`, während mehrere Spalten als Array in eckige Klammern geschrieben werden müssen (z.B. `groupby(['Spalte1', 'Spalte2'])`).\n","\n","In der `agg()`-Methode können die Aggregationen auf verschiedene Weisen angegeben werden. Die folgende Form erlaubt es, für mehrere Spalten verschiedene Aggregationen anzugeben und den daraus resultierenden Spalten explizite Namen zuzuweisen: `agg(aggregierte_spalte1=(\"quellspalte1\", \"aggregatfunktion\"), aggregierte_spalte2=(\"quellspalte2\", \"aggregatfunktion\"))`.\n","\n","Beispiel:\n","* `dataframe.agg(erloes_sum=(\"erloes\", \"sum\"), erloes_avg=(\"erloes\", \"mean\"))"],"metadata":{"id":"P74YTT46bICQ"}},{"cell_type":"markdown","source":["### Datensätze aggregieren"],"metadata":{"id":"8JCsab0aUBTD"}},{"cell_type":"code","source":["# Patiententabelle nach Geschlecht gruppieren und die Anzahl der Datensätze zählen\n","df_patient.groupby(\"patient_geschlecht\").agg(n=(\"patient_geschlecht\", \"size\"))"],"metadata":{"id":"b1F3bhOgcHra"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Fälletabelle nach Fallart gruppieren und die Fallzahl, den Mittelwert der Liegedauer sowie Summe der Erlöse aggregieren\n","df_faelle.groupby(\"fallart_id\").agg(fallzahl=(\"fallart_id\", \"size\"), liegedauer_avg=(\"liegedauer_tage\", \"mean\"), erloes_sum=(\"erloes_fallpauschale\", \"sum\"))"],"metadata":{"id":"M7k9Y_W6UZWB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Filterung bei Aggregatfunktionen\n","\n","Zur `HAVING`-Klausel aus SQL gibt es kein eigenes Äquivalent, stattdessen kann einfach die oben beschriebene `query()`-Methode in der Pipeline hinter die Aggregation gestellt werden.\n","\n","Da die Pipelines bei vielen Verarbeitungsschritten unübersichtlich werden können, ist es möglich, die Schritte auf mehrere Zeilen aufzuteilen. Ans Ende der jeweils vorherigen Zeile muss ein Backslash (`\\`) gestellt werden und die fortsetzenden Zeilen müssen eingerückt werden."],"metadata":{"id":"q03tszPfeKYW"}},{"cell_type":"code","source":["# Fälletabelle nach Fallart gruppieren und die Fallzahl, den Mittelwert der Liegedauer sowie Summe der Erlöse aggregieren\n","# - Filterung nach der Aggregation auf Einträge mit mittlerer Liegedauer > 6 Tage\n","df_faelle.groupby(\"fallart_id\") \\\n","  .agg(fallzahl=(\"fallart_id\", \"size\"), liegedauer_avg=(\"liegedauer_tage\", \"mean\"), erloes_sum=(\"erloes_fallpauschale\", \"sum\")) \\\n","  .query(\"liegedauer_avg > 6\")"],"metadata":{"id":"3P_oMBs5dZWS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Zusammenführen mehrerer Dataframes\n","\n","Das Gegenstück zur `JOIN`-Klausel in SQL ist die `merge()`-Methode in Pandas, die ebenfalls inner sowie left/right outer Joins über eine oder mehrere Spalten der einbezogenen Dataframes erlaubt.\n","\n","Die `merge()`-Methode kann in einer Pipeline an den ersten (\"linken\") in den Join einbezogenen Dataframe angehängt werden und enthält als erstes Argument den \"rechts\" zu verbindenen Dataframe: `dataframe1.merge(dataframe2)`.\n","\n","Mit dem `on´-Argument werden die Spalten festgelegt, die für die Verknüpfung der Dataframes eingesetzt werden sollen:\n","* `on='Spalte1'`: es soll nur über Spalte1 gejoined werden, die in beiden Dataframes identisch benannt ist\n","* `on=['Spalte1', 'Spalte2']`: es soll über Spalte1 und Spalte2 gejoined werden, die in beiden Dataframes identisch benannt sind\n","* `left_on=['LinkeSpalte1', 'LinkeSpalte2'], right_on=[: 'RechteSpalte1', 'RechteSpalte2']`: wenn die Spalten der Join-Kriterien in den beiden Dataframes unterschiedlich benannt sind, müssen sie separat in `left_on`und `right_on`-Argumenten in identischer Reihenfolge angegeben werden.\n","\n","Identisch benannte Spalten, die als Join-Kriterien angegeben wurden, werden im Ergebnisdataframe zusammengeführt, während unterschiedlich benannte Spalten übernommen werden. Identisch benannte Spalten außerhalb der Join-Kriterien werden separat in den Ergebnisdataframe benommen und ggf. mit einem Suffix qualifiziert.\n","\n","\"Überflüssige\" Spalten, die z.B. wegen Redundanzen zwischen den Tabellen im Ergebnisdataframe enthalten sind, können mit der `drop()`-Methode entfernt werden.\n","\n"],"metadata":{"id":"C3DfPcwkUrR8"}},{"cell_type":"markdown","source":["### INNER JOIN: Schnittmenge von Dataframes bilden\n","\n"],"metadata":{"id":"09uLxupAYB7o"}},{"cell_type":"code","source":["# Spalten für Fall-ID und Fallart-ID aus dem Dataframe df_faelle selektieren und\n","# den Fallartbezeichner per merge() hinzufügen\n","df_faelle[[\"fall_id\", \"fallart_id\"]].merge(df_fallart, on=\"fallart_id\").head()"],"metadata":{"id":"f9ZbXQOnUsaa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Diagnosebezeichner zu den Fällen ergänzen\n","df_faelle[[\"fall_id\", \"hauptdiagnose_snomed_id\"]] \\\n","  .merge(df_diagnose[[\"snomed_id\", \"snomed_name\"]], left_on=\"hauptdiagnose_snomed_id\", right_on=\"snomed_id\") \\\n","  .head()"],"metadata":{"id":"ktPxatraTpcW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Pipeline für Aggregation & Zählung der Fälle nach Fallart und Ergänzung der Fallartbezeichner\n","df_faelle.merge(df_fallart, on=\"fallart_id\") \\\n","  .groupby([\"fallart_id\", \"fallart_name\"]) \\\n","  .agg({\"fallart_id\": \"size\"})"],"metadata":{"id":"dNWOWn_WPpgT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### OUTER JOINs: Datensätze einbeziehen, für die es keine Entsprechung in beiden Dataframes gibt\n","\n","Mit dem `how`-Argument kann festgelegt werden, ob ein inner (default), left oder right outer join verwendet werden soll."],"metadata":{"id":"9zhGREozZpJG"}},{"cell_type":"code","source":["# Fälle mit Bezeichnern für ihre Hauptdiagnosen ausgeben, aber Fälle ohne Hauptdiagnose beibehalten\n","df_faelle[[\"fall_id\", \"hauptdiagnose_snomed_id\"]] \\\n","  .merge(df_diagnose[[\"snomed_id\", \"snomed_name\"]], left_on=\"hauptdiagnose_snomed_id\", right_on=\"snomed_id\", how=\"left\") \\\n","  .drop(columns=\"snomed_id\") \\\n","  .head()"],"metadata":{"id":"90HFlF3AZpTX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Mehrere  Dataframes \"hintereinander\" zusammenfügen\n","\n","Neben dem \"horizontalen\" Verknüpfen von Dataframes mit der `merge()`-Methode können Dataframes auch mit der `concat()`-Funktion \"hintereinander gestellt\" werden. Die Spalten beides Datensätze müssen dafür identische Namen und Spalten haben.\n","\n","Im Gegensatz zu den oben beschriebenen Methoden ist `concat()` eine eigenständige Funktion, der beide (oder auch mehrere) hintereinanderzusetzende Dataframes als Argument übergeben werden müssen, und keine Methode eines einzelnen Dataframe.\n","\n","Im Gegensatz zum `UNION`-Statement in SQL werden hier beide Datensätze vollständig hintereinander gesetzt, unabhängig davon, ob es doppelte Datensätze gibt. Die `concat()`-Methode enspricht also einem `UNION ALL`-Statement in SQL. Falls eindeutige Datensätze benötigt werden, können mit der `drop_duplicates()`-Methode Zeilen mit identischen Inhalten zusammengeführt werden."],"metadata":{"id":"mtaLmqptVjej"}},{"cell_type":"code","source":["# Zwei Datensätze mit Teilen des Diagnosehierarchie selektieren (Bronchitis und Diabetes)\n","df_bronchitis = df_diagnose.query(\"snomed_name.str.contains('bronchitis')\")\n","df_diabetes   = df_diagnose.query(\"snomed_name.str.contains('diabetes')\")\n","\n","# Beide Datensätze mit concat() zusammenfügen\n","pandas.concat([df_bronchitis, df_diabetes])"],"metadata":{"id":"TCgaGthWVkBj"},"execution_count":null,"outputs":[]}]}